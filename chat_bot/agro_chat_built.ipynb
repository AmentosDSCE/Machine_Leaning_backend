{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "219909ff-d480-4a66-8083-4c7e55d906b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "import pickle\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6d2faeba-654e-4ebd-ac8e-21bbcd84533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4fb55d63-029c-4d4b-ab2b-c332c19a7cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1866b2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\dheer\\anaconda3\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.5)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (63.4.1)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (23.3.3)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.6)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.51.3)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\dheer\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7a444d02-f84f-45af-ab4b-be6eb8f837e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3ff1043e-37ee-47a5-ae67-6b43beb6c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fd86856a-7228-44e3-a3ab-374e15b81fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = json.loads(open('intents.json').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6b27178f-d1e5-4310-b8d9-1ab065432b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "classes=[]\n",
    "documents=[]\n",
    "ignore_letters = ['?', '!', '.', ',']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "73b534a8-ce82-4b7e-9d91-8d2a3f7b789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in intents['intents'] :\n",
    "    for pattern in intent['patterns']:\n",
    "        word_list = nltk.word_tokenize(pattern)\n",
    "        words.extend(word_list)\n",
    "        documents.append((word_list, intent['tag']))\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d4869c63-a684-4802-97f6-080374ac483d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dheer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "db171853-6951-4746-8054-07729df8b347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'hello',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'howdy',\n",
       " 'what',\n",
       " 'is',\n",
       " 'your',\n",
       " 'name',\n",
       " 'what',\n",
       " 'can',\n",
       " 'i',\n",
       " 'call',\n",
       " 'you',\n",
       " 'can',\n",
       " 'you',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'your',\n",
       " 'name',\n",
       " 'may',\n",
       " 'i',\n",
       " 'know',\n",
       " 'farmer',\n",
       " 'plans',\n",
       " 'farmer',\n",
       " 'plans',\n",
       " 'planning',\n",
       " 'financing',\n",
       " 'plans',\n",
       " 'I',\n",
       " 'am',\n",
       " 'going',\n",
       " 'to',\n",
       " 'die',\n",
       " 'I',\n",
       " 'need',\n",
       " 'some',\n",
       " 'emotional',\n",
       " 'support',\n",
       " 'please',\n",
       " 'help',\n",
       " 'me',\n",
       " 'how',\n",
       " 'can',\n",
       " 'i',\n",
       " 'navigate',\n",
       " 'in',\n",
       " 'you',\n",
       " '?',\n",
       " 'how',\n",
       " 'can',\n",
       " 'i',\n",
       " 'get',\n",
       " 'full',\n",
       " 'use',\n",
       " 'from',\n",
       " 'you',\n",
       " '?',\n",
       " 'what',\n",
       " 'can',\n",
       " 'you',\n",
       " 'do',\n",
       " '?',\n",
       " 'thank',\n",
       " 'you',\n",
       " 'see',\n",
       " 'you',\n",
       " '!',\n",
       " '!',\n",
       " 'bye',\n",
       " 'good',\n",
       " 'bye']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c2be0d8a-a8eb-47d7-a878-8f14fd6ea807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['hi'], 'greetings'),\n",
       " (['hello'], 'greetings'),\n",
       " (['how', 'are', 'you', '?'], 'greetings'),\n",
       " (['howdy'], 'greetings'),\n",
       " (['what', 'is', 'your', 'name'], 'name'),\n",
       " (['what', 'can', 'i', 'call', 'you'], 'name'),\n",
       " (['can', 'you', 'tell', 'me', 'your', 'name'], 'name'),\n",
       " (['may', 'i', 'know', 'farmer', 'plans'], 'plans'),\n",
       " (['farmer', 'plans'], 'plans'),\n",
       " (['planning'], 'plans'),\n",
       " (['financing', 'plans'], 'plans'),\n",
       " (['I', 'am', 'going', 'to', 'die'], 'emotional support'),\n",
       " (['I', 'need', 'some', 'emotional', 'support'], 'emotional support'),\n",
       " (['please', 'help', 'me'], 'emotional support'),\n",
       " (['how', 'can', 'i', 'navigate', 'in', 'you', '?'], 'use'),\n",
       " (['how', 'can', 'i', 'get', 'full', 'use', 'from', 'you', '?'], 'use'),\n",
       " (['what', 'can', 'you', 'do', '?'], 'use'),\n",
       " (['thank', 'you', 'see', 'you', '!', '!'], 'end'),\n",
       " (['bye'], 'end'),\n",
       " (['good', 'bye'], 'end')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9e7c6d7c-55e9-406d-aa25-d517ac31f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c7639baf-d7c0-49a8-aff1-d67e6d05f253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dheer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "67d48644-77b9-4737-8451-891109bc3302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\dheer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2d8c156a-d221-42af-b1a4-5ea6c423e71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sorted(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1d652c2d-231d-435e-bc27-b6fc17bf6c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'are',\n",
       " 'bye',\n",
       " 'call',\n",
       " 'can',\n",
       " 'die',\n",
       " 'do',\n",
       " 'emotional',\n",
       " 'farmer',\n",
       " 'financing',\n",
       " 'from',\n",
       " 'full',\n",
       " 'get',\n",
       " 'going',\n",
       " 'good',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hi',\n",
       " 'how',\n",
       " 'howdy',\n",
       " 'i',\n",
       " 'in',\n",
       " 'is',\n",
       " 'know',\n",
       " 'may',\n",
       " 'me',\n",
       " 'name',\n",
       " 'navigate',\n",
       " 'need',\n",
       " 'plan',\n",
       " 'planning',\n",
       " 'please',\n",
       " 'see',\n",
       " 'some',\n",
       " 'support',\n",
       " 'tell',\n",
       " 'thank',\n",
       " 'to',\n",
       " 'use',\n",
       " 'what',\n",
       " 'you',\n",
       " 'your']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4cc3af43-6ea8-45da-889d-f99da7e8f849",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted(set(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "552d07fd-bc3c-49ea-bfaa-532cd01b82aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(words, open('words.pkl', 'wb'))\n",
    "pickle.dump(classes, open('classes.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "900dbaf8-6808-436b-b953-3929d6ce7bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training =[]\n",
    "output_empty =[0]*  len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f1acd6ef-4c42-4c23-b4eb-5402ca4127bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in documents:\n",
    "    bag =[]\n",
    "    word_patterns = document[0]\n",
    "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
    "    for word in words:\n",
    "        bag.append(1) if word in word_patterns else bag.append(0)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(document[1])]=1\n",
    "    training.append([bag, output_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "10cbb31e-b0b0-4fa8-9516-f9c6975c634b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dheer\\AppData\\Local\\Temp\\ipykernel_4992\\4111924452.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training=np.array(training)\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(training)\n",
    "training=np.array(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4b16db1c-770d-4948-9424-0531c53dc40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = list(training[:, 0])\n",
    "train_y = list(training[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c04215c4-06bc-4ad7-baa5-3c674d7fe6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),) ,activation= 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1fe944a4-b4c6-4a40-8d8c-152119ecebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cea844e9-11be-4f85-a8a6-87f56fd20568",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d33f1498-59a5-4472-8c95-6ffa5983eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "038fc5e1-afbf-4c78-bf97-c1d7a10cc23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 958ms/step - loss: 8.9691 - accuracy: 0.1500\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.2874 - accuracy: 0.1500\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.3337 - accuracy: 0.1500\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 9.1910 - accuracy: 0.1500\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 10.4746 - accuracy: 0.1500\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.2688 - accuracy: 0.1500\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 8.9960 - accuracy: 0.1500\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.3714 - accuracy: 0.1500\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.3354 - accuracy: 0.1500\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 5.6493 - accuracy: 0.1500\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.0275 - accuracy: 0.1500\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.1451 - accuracy: 0.1500\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.2075 - accuracy: 0.1500\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.0928 - accuracy: 0.1500\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.0587 - accuracy: 0.1500\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.0740 - accuracy: 0.1500\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.1131 - accuracy: 0.1500\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.9800 - accuracy: 0.1500\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.9634 - accuracy: 0.1500\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.9298 - accuracy: 0.1500\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.9304 - accuracy: 0.1500\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.9705 - accuracy: 0.1500\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8414 - accuracy: 0.1500\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.9744 - accuracy: 0.1500\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.9609 - accuracy: 0.1000\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.9527 - accuracy: 0.1500\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.9929 - accuracy: 0.2000\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.3505 - accuracy: 0.1500\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.8607 - accuracy: 0.2000\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.8748 - accuracy: 0.1500\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.9633 - accuracy: 0.1500\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.2748 - accuracy: 0.1500\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.0033 - accuracy: 0.1500\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.8889 - accuracy: 0.1500\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.9698 - accuracy: 0.1500\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.8838 - accuracy: 0.1500\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.8281 - accuracy: 0.2500\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3.2559 - accuracy: 0.1500\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.9575 - accuracy: 0.1000\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.6690 - accuracy: 0.1500\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.8948 - accuracy: 0.1500\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.8908 - accuracy: 0.1000\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.2651 - accuracy: 0.2000\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.3547 - accuracy: 0.1500\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.2019 - accuracy: 0.2000\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2735 - accuracy: 0.1500\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.3486 - accuracy: 0.1500\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.1742 - accuracy: 0.2000\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.9985 - accuracy: 0.1500\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.8880 - accuracy: 0.1500\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.1163 - accuracy: 0.1000\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.9784 - accuracy: 0.2500\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.9832 - accuracy: 0.2500\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.5913 - accuracy: 0.3000\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.7116 - accuracy: 0.0500\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.1193 - accuracy: 0.2000\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0120 - accuracy: 0.1000\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.9065 - accuracy: 0.2000\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0706 - accuracy: 0.1000\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9568 - accuracy: 0.2000\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0526 - accuracy: 0.1500\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0090 - accuracy: 0.1500\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8918 - accuracy: 0.2000\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.8228 - accuracy: 0.1500\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.9596 - accuracy: 0.1000\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.8193 - accuracy: 0.2500\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.9130 - accuracy: 0.2000\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.8719 - accuracy: 0.1500\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.8030 - accuracy: 0.2000\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.8349 - accuracy: 0.2000\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.8383 - accuracy: 0.1500\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.5389 - accuracy: 0.1000\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9682 - accuracy: 0.1000\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.8324 - accuracy: 0.1500\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7626 - accuracy: 0.2500\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.8239 - accuracy: 0.1500\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9022 - accuracy: 0.2000\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.8573 - accuracy: 0.1000\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7828 - accuracy: 0.2000\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8208 - accuracy: 0.1500\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.8190 - accuracy: 0.2000\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7695 - accuracy: 0.2500\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.8142 - accuracy: 0.1000\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9088 - accuracy: 0.1500\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8224 - accuracy: 0.1000\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8605 - accuracy: 0.1000\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.9087 - accuracy: 0.1000\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.8757 - accuracy: 0.1500\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.8902 - accuracy: 0.1000\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.7503 - accuracy: 0.2000\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.8089 - accuracy: 0.1000\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.7645 - accuracy: 0.1500\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.8250 - accuracy: 0.1500\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.8395 - accuracy: 0.2000\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8862 - accuracy: 0.2500\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.7443 - accuracy: 0.3000\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8746 - accuracy: 0.1500\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 517us/step - loss: 1.8148 - accuracy: 0.2500\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.7783 - accuracy: 0.1500\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.8460 - accuracy: 0.1000\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.8581 - accuracy: 0.1500\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.7955 - accuracy: 0.1500\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8170 - accuracy: 0.2000\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7483 - accuracy: 0.2000\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.7961 - accuracy: 0.1500\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.8519 - accuracy: 0.0500\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7993 - accuracy: 0.1500\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8479 - accuracy: 0.1000\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.8447 - accuracy: 0.1500\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.9009 - accuracy: 0.1000\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.8451 - accuracy: 0.1000\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.8458 - accuracy: 0.1000\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.8994 - accuracy: 0.1000\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.7524 - accuracy: 0.1500\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.8051 - accuracy: 0.2000\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7646 - accuracy: 0.2000\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7740 - accuracy: 0.2500\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.8355 - accuracy: 0.1000\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.7980 - accuracy: 0.1500\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.7841 - accuracy: 0.1500\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.7965 - accuracy: 0.1500\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.7703 - accuracy: 0.3000\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7809 - accuracy: 0.2500\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.8502 - accuracy: 0.1500\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.8297 - accuracy: 0.0500\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7533 - accuracy: 0.2000\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.7872 - accuracy: 0.2000\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.8802 - accuracy: 0.2000\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.7734 - accuracy: 0.2000\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.8514 - accuracy: 0.1500\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.8445 - accuracy: 0.1500\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.8014 - accuracy: 0.1000\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7922 - accuracy: 0.2500\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7873 - accuracy: 0.2000\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.8292 - accuracy: 0.2000\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.8742 - accuracy: 0.2000\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.8142 - accuracy: 0.1500\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7123 - accuracy: 0.1500\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7805 - accuracy: 0.2500\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.7648 - accuracy: 0.2000\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.8502 - accuracy: 0.1000\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.7907 - accuracy: 0.2000\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.8975 - accuracy: 0.1500\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.7232 - accuracy: 0.1500\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8610 - accuracy: 0.1000\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8518 - accuracy: 0.1500\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.8282 - accuracy: 0.1500\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7355 - accuracy: 0.2000\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.8076 - accuracy: 0.1000\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8112 - accuracy: 0.2000\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.7980 - accuracy: 0.2000\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.8962 - accuracy: 0.0500\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8122 - accuracy: 0.2000\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.8014 - accuracy: 0.0500\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8312 - accuracy: 0.2000\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8077 - accuracy: 0.1000\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.8706 - accuracy: 0.1000\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7967 - accuracy: 0.2000\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.7780 - accuracy: 0.2000\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.8539 - accuracy: 0.1000\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.8440 - accuracy: 0.1500\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7827 - accuracy: 0.2000\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8091 - accuracy: 0.2000\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7742 - accuracy: 0.2000\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7465 - accuracy: 0.2500\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.7714 - accuracy: 0.2000\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.8639 - accuracy: 0.1000\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.8173 - accuracy: 0.1000\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.8399 - accuracy: 0.1000\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8088 - accuracy: 0.1000\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7451 - accuracy: 0.2000\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.8089 - accuracy: 0.1000\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.7596 - accuracy: 0.2500\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7940 - accuracy: 0.1500\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7252 - accuracy: 0.4000\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.8541 - accuracy: 0.1000\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.7397 - accuracy: 0.1500\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.8266 - accuracy: 0.1000\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.7859 - accuracy: 0.1500\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.8008 - accuracy: 0.1000\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8019 - accuracy: 0.2000\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.8286 - accuracy: 0.2000\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9514 - accuracy: 0.2000\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.8057 - accuracy: 0.1500\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.8004 - accuracy: 0.1500\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9634 - accuracy: 0.0500\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.7722 - accuracy: 0.2000\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.7775 - accuracy: 0.2000\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.7793 - accuracy: 0.1500\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.8453 - accuracy: 0.1000\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7720 - accuracy: 0.1000\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.7316 - accuracy: 0.2500\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.8142 - accuracy: 0.2000\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.0249 - accuracy: 0.1000\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7808 - accuracy: 0.2000\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.7589 - accuracy: 0.2500\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7793 - accuracy: 0.2000\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.7484 - accuracy: 0.2500\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.7588 - accuracy: 0.2000\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8310 - accuracy: 0.1500\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(np.array(train_x), np.array(train_y),epochs=200, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "229f0420-e14c-40e6-908a-e9f491c1510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('chatbot_model.h5', hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8751e361-9ce8-4b86-97fa-88d644f2b34b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b576d1c-4a6b-427c-847d-bcc475763e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7011f576-a279-451a-a4d9-afa0a8565194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3600c436-2eb9-4b5c-bd4e-96eddaa9f910",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9606e17e-32a2-4911-b0d6-8e9867277434",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pickle.load(open('words.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6387b773-f69d-4385-a64e-35d8fd30fa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes =  pickle.load(open('classes.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "69471205-80e7-4530-ade3-e307f8a209e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"chatbot_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c5a2b244-1269-45c7-b8b4-6fcfc31a7f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    sentence_words=[lemmatizer.lemmatize(word) for word in sentence_words]\n",
    "    return sentence_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "45bdb3d0-cd09-43c7-85b6-ed82c989625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(sentence):\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    bag =[0]*len(words)\n",
    "    \n",
    "    for w in sentence_words:\n",
    "        for i, word in enumerate(words):\n",
    "            if word==w:\n",
    "                bag[i]=1\n",
    "    return np.array(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cf814c16-446f-497a-8822-2af3e55b9de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(sentence):\n",
    "    bow = bag_of_words(sentence)\n",
    "    res= model.predict(np.array([bow]))[0]\n",
    "    ERROR_THRESHOLD=0.25\n",
    "    results =[[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
    "    \n",
    "    results.sort(key= lambda x:x[1], reverse=True)\n",
    "    \n",
    "    return_list =[]\n",
    "    \n",
    "    for r in results:\n",
    "        return_list.append({'intent':classes[r[0]], 'probability': str(r[1])})\n",
    "    return return_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c1ecef02-ca07-4200-9175-8715b3ced3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(intents_list, intents_json):\n",
    "    tag = intents_list[0]['intent']\n",
    "    list_of_intents=intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if i['tag']==tag:\n",
    "            result= random.choice(i['responses'])\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "96d90690-783b-4811-94d2-ec8da1190123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 120ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'thanks for using our service hope we see you again'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = \"can i use service\"\n",
    "ints = predict_class(message)\n",
    "get_response(ints, intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4e2eb0df-4f0b-4f78-8be4-d977b333ccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = get_response(ints, intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fde17f3f-8ccf-47da-a0a0-48592cc73470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8b0a40e5-e3fd-4e1a-99ec-ccad25043099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'check out our services might help you in some way'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf86a84-f6a4-4e4e-9481-697904f52496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
